# Debugging GemFire Applications

This article provides some practical suggestions for debugging GemFire Applications.

## GemFire Artifacts
* clusterConfig and cache.xml - provided by the application developer to configure the caches and regions for GemFire client and server processes.
* properties files - provided by the application developer to configure system properties and membership discovery
* System logs - logs generated by GemFire clients, servers, and locators. The logs contain information about membership, client connections, warnings about outstanding message requests and errors.  The log also displays the system properties.
* Statistics - archive files generated by GemFire clients and/or servers containing information about the GemFire application. GemFire VSD (Visual Statistics Display) is used to graph the GemFire and system metrics recorded in these archives. 

## GemFire Shell (gfsh)
GemFire gfsh provides a command-line interface from which you can launch, manage, and monitor GemFire processes, data, and applications.  The shell provides commands useful in debugging and to bring all the information to a single location for analysis.  The following gfsh commands require first executing the gfsh ```connect``` command to establish the connection to the locator or JMX Manager of the distributed system.
* export logs
* export stack-traces
* show log
* show dead-locks

## General Guidelines
1. Check your environment - Machine (O/S and System settings), JDK, JVM properties (-Xmx -Xms), GC parameters  

1. Draw out a diagram of your system topology (servers, clients) and make a note of Listeners, Writers and other plug-ins.  

1. Verify your cache and region configuration  

1. Confirm your system properties (Review properties files and display in system log)  

1. On your system topology diagram, add notes on the initialization and processing being done in various members or classes of members.    

1. If you are debugging a specific interaction, draw a sequence diagram.

## Specific search strings and patterns
1. If possible, bring all the system logs and stack dumps together into a single directory for inspection (use gfsh commands above).  

1. Search the system logs for warning, error or severe messages  

1. Search the system logs for any underlying Exceptions (perhaps thrown from user supplied plug-ins).  For example: ConcurrentModificationException, NullPointerException, SerializationException.  

1. Search the system logs for warnings about resources causing delays in statistics sampling.  If found, use VSD to investigate further.    

    ```
    [warning 2015/03/29 04:47:23.028 PDT gemfire1_w1-gst-dev26_15651 <Thread-5 StatSampler> tid=0x55] Statistics 
sampling thread detected a wakeup delay of 8,310 ms, indicating a possible resource issue. Check the GC,
memory, and CPU statistics.
    ```  

1. Verify there are no HotSpot (hs_err_pid.log files) indicating a HotSpot error in the JVM
Refer to the [Oracle Troubleshooting Guide](http://www.oracle.com/technetwork/java/javase/crashes-137240.html) for more details.  

1. Verify that there are no heapdump (*.hprof) files or OutOfMemoryErrors.
Tools such as [jhat](http://docs.oracle.com/javase/7/docs/technotes/tools/share/jhat.html) and [Eclipse Memory Analyzer](https://eclipse.org/mat/) can provide heap histograms and leak suspects.  

1. Search the stack dumps for ```Java-level deadlock```.  Dumping the stacks using [jstack](https://docs.oracle.com/javase/7/docs/webnotes/tsg/TSG-VM/html/hangloop.html) or the Linux command ```kill -3 <pid>``` will highlight any Java-level deadlocks including the threads involved in the deadlock as well as the stack dumps for each of those threads.  When debugging, it is best to get stack dumps for all VMs.  To determine if progress is being made, execute multiple thread dumps several seconds apart for comparison.   

1. Search the system logs for any ```15 seconds have elapsed``` messages which don't have corresponding ```wait for replies has completed``` logs.  You can match these log messages together via the thread id or native thread id.  Note that these messages are only logged between peers in the Distributed System.  See "Special Considerations for Clients" for messages specific to GemFire clients.  

    In this example, we can see that the request did complete, so while we should be concerned (and possibly check stats in vsd to see what system resources are causing this delay), it will not be the cause of our hang.    

    ```
    [warning 2014/07/26 02:02:54.384 PDT dataStoregemfire5_w1-gst-dev12_25904 <Pooled Waiting Message Processor 11> tid=0xb5] 
    15 seconds have elapsed while waiting for replies: <DeposePrimaryBucketMessage$DeposePrimaryBucketResponse 2308 
    waiting for 1 replies from [10.138.44.112(dataStoregemfire2_w1-gst-dev12_37336:37336)<v82>:52253]> 
on 10.138.44.112(dataStoregemfire5_w1-gst-dev12_25904:25904)<v82>:14125 
whose current membership list is: [[10.138.44.112(dataStoregemfire4_w1-gst-dev12_14688:14688)<v82>:31076, 10.138.44.112(accessorgemfire1_w1-gst-dev12_37800:37800)<v81>:59828, 10.138.44.112(accessorgemfire3_w1-gst-dev12_6348:6348)<v81>:16162, 10.138.44.112(dataStoregemfire2_w1-gst-dev12_37336:37336)<v82>:52253, 10.138.44.112(accessorgemfire4_w1-gst-dev12_24644:24644)<v81>:64839, 10.138.44.112(accessorgemfire2_w1-gst-dev12_40732:40732)<v81>:54553, 10.138.44.112(dataStoregemfire3_w1-gst-dev12_3060:3060)<v82>:43488, 10.138.44.112(accessorgemfire5_w1-gst-dev12_25924:25924)<v81>:56180,10.138.44.112(dataStoregemfire5_w1-gst-dev12_25904:25904)<v82>:14125, 10.138.44.112(locatorgemfire1_w1-gst-dev12_50584:50584:locator)<ec><v0>:16639, 10.138.44.112(dataStoregemfire1_w1-gst-dev12_31492:31492)<v82>:40311]]

    [info 2014/07/26 02:03:00.437 PDT dataStoregemfire5_w1-gst-dev12_25904 <Pooled Waiting Message Processor 11> tid=0xb5] 
    DeposePrimaryBucketMessage$DeposePrimaryBucketResponse wait for replies completed
    ```  

If the request is never satisfied (there is no corresponding ```wait for replies completed```), look at the stack dumps for the non-responsive member.  There could be a Java-level deadlock within that vm.  

There can also be distributed deadlocks between members.  This requires following the ```15 seconds have elapsed``` warnings to the remote vms and looking at the stack dumps.  Searching for ```waiting to lock``` in the stack dumps can also help to identify the problematic vm.  Once found in a non-responsive member, find the thread in that vm that holds the lock and determine what prevents it from releasing the lock.    

This example shows the outstanding request from the system log and the relevant stack dumps from the non-responding vm.

The system log shows that vm 12659 is still awaiting a response from vm 12706  
```
dataStoregemfire1_12659/system.log:[warning 2014/03/04 08:18:27.954 PST dataStoregemfire1_w1-gst-dev25_12659 <ResourceManagerRecoveryThread> tid=0x60] 
15 seconds have elapsed while waiting for replies: <ManageBucketMessage$NodeResponse 6407 
waiting for 1 replies from [10.138.44.125(dataStoregemfire3_w1-gst-dev25_12706:12706)<v315>:2225]> on 10.138.44.125(dataStoregemfire1_w1-gst-dev25_12659:12659)<v314>:55872 
whose current membership list is: [[10.138.44.125(dataStoregemfire2_w1-gst-dev25_12480:12480)<v312>:24218, 10.138.44.125(locatorgemfire1_w1-gst-dev25_22478:22478:locator)<ec><v0>:13070, 10.138.44.125(accessorgemfire3_w1-gst-dev25_12501:12501)<v313>:18675, 10.138.44.125(dataStoregemfire1_w1-gst-dev25_12659:12659)<v314>:55872, 10.138.44.125(dataStoregemfire5_w1-gst-dev25_12483:12483)<v312>:27445, 10.138.44.125(accessorgemfire1_w1-gst-dev25_12607:12607)<v315>:46692, 10.138.44.125(dataStoregemfire4_w1-gst-dev25_12500:12500)<v312>:48232, 10.138.44.125(accessorgemfire2_w1-gst-dev25_12705:12705)<v316>:11732, 10.138.44.125(dataStoregemfire3_w1-gst-dev25_12706:12706)<v315>:2225]]
```

The stack dumps from 12706 show the ```waiting to lock <monitor>``` and ```locked <monitor>``` in the stack dumps  
```
"Pooled Waiting Message Processor 12" daemon prio=10 tid=0xdf040000 nid=0x383f waiting for monitor entry [0xd960b000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at com.gemstone.gemfire.internal.cache.PartitionedRegionDataStore.grabFreeBucket(PartitionedRegionDataStore.java:424)
        - waiting to lock <0xe47a7fa8> (a com.gemstone.gemfire.internal.cache.ProxyBucketRegion)
        at com.gemstone.gemfire.internal.cache.PartitionedRegionDataStore.grabBucket(PartitionedRegionDataStore.java:2940)
        at com.gemstone.gemfire.internal.cache.PartitionedRegionDataStore.handleManageBucketRequest(PartitionedRegionDataStore.java:1032)
        at com.gemstone.gemfire.internal.cache.partitioned.ManageBucketMessage.operateOnPartitionedRegion(ManageBucketMessage.java:123)
        at com.gemstone.gemfire.internal.cache.partitioned.PartitionMessage.process(PartitionMessage.java:297)
        at com.gemstone.gemfire.distributed.internal.DistributionMessage.scheduleAction(DistributionMessage.java:357)
        at com.gemstone.gemfire.distributed.internal.DistributionMessage$1.run(DistributionMessage.java:420)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at com.gemstone.gemfire.distributed.internal.DistributionManager.runUntilShutdown(DistributionManager.java:720)
        at com.gemstone.gemfire.distributed.internal.DistributionManager$6$1.run(DistributionManager.java:1057)
        at java.lang.Thread.run(Thread.java:662)

"Recovery thread for bucket _B__PR__1_46" daemon prio=10 tid=0xdc965400 nid=0x36dd in Object.wait() [0xdb3b7000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at com.gemstone.gemfire.internal.cache.persistence.PersistenceAdvisorImpl$MembershipChangeListener.waitForChange(PersistenceAdvisorImpl.java:1047)
        - locked <0xe6223758> (a com.gemstone.gemfire.internal.cache.persistence.PersistenceAdvisorImpl$MembershipChangeListener)
        at com.gemstone.gemfire.internal.cache.persistence.PersistenceAdvisorImpl.getInitialImageAdvice(PersistenceAdvisorImpl.java:820)
        at com.gemstone.gemfire.internal.cache.persistence.CreatePersistentRegionProcessor.getInitialImageAdvice(CreatePersistentRegionProcessor.java:47)
        at com.gemstone.gemfire.internal.cache.DistributedRegion.getInitialImageAndRecovery(DistributedRegion.java:1377)
        at com.gemstone.gemfire.internal.cache.DistributedRegion.initialize(DistributedRegion.java:1196)
        at com.gemstone.gemfire.internal.cache.BucketRegion.initialize(BucketRegion.java:264)
        at com.gemstone.gemfire.internal.cache.LocalRegion.createSubregion(LocalRegion.java:1229)
        at com.gemstone.gemfire.internal.cache.PartitionedRegionDataStore.createBucketRegion(PartitionedRegionDataStore.java:742)
        at com.gemstone.gemfire.internal.cache.PartitionedRegionDataStore.grabFreeBucket(PartitionedRegionDataStore.java:451)
        - locked <0xe47a7fa8> (a com.gemstone.gemfire.internal.cache.ProxyBucketRegion)
        at com.gemstone.gemfire.internal.cache.PartitionedRegionDataStore.grabFreeBucketRecursively(PartitionedRegionDataStore.java:301)
        at com.gemstone.gemfire.internal.cache.PartitionedRegionDataStore.grabBucket(PartitionedRegionDataStore.java:2956)
        at com.gemstone.gemfire.internal.cache.ProxyBucketRegion.recoverFromDisk(ProxyBucketRegion.java:417)
        at com.gemstone.gemfire.internal.cache.ProxyBucketRegion.recoverFromDiskRecursively(ProxyBucketRegion.java:386)
        at com.gemstone.gemfire.internal.cache.PRHARedundancyProvider$5.run2(PRHARedundancyProvider.java:1934)
        at com.gemstone.gemfire.internal.cache.PRHARedundancyProvider$RecoveryRunnable.run(PRHARedundancyProvider.java:2243)
        at com.gemstone.gemfire.internal.cache.PRHARedundancyProvider$5.run(PRHARedundancyProvider.java:1926)
        at java.lang.Thread.run(Thread.java:662)
```

## Special considerations for GemFire clients
GemFire clients can fail with ServerConnectivityExceptions when the servers are too busy to handle client requests.  For example, with large GC pauses or distributed deadlocks.  

Look for ```is being terminated because its client timeout``` messages in the server system logs and to determine whether or not this is occurring in your application.  If so, review the server side system logs, stack dumps and statistics to determine the cause.
```
[warning 2015/03/14 06:00:15.062 PDT bridgegemfire_1_3_w1-gst-dev08_25525 <ClientHealthMonitor Thread> tid=0x54] 
Server connection from [identity(w1-gst-dev08(edgegemfire_1_1_w1-gst-dev08_25635:25635:loner):39198:65125d18:edgegemfire_1_1_w1-gst-dev08_25635,connection=1; port=49383] 
is being terminated because its client timeout of 30,000 has expired.

```

## Improving traceability during initial development

### Function Execution
To trace function execution from the initiator to the vm executing the function, pass the initiating thread id to the function using ```withArgs``` and log in both vms.  Of course, this could easily be a string containing the pid, DistributedMemberId or any other identifying information for your application.
```
    ArrayList aList = new ArrayList();
    aList.add("myFunctionOperation");
    aList.add(Thread.currentThread().getId());
    System.out.println("Executing " + aList.get(0) + " from " + aList.get(1));
    ResultCollector drc = dataSet.withFilter(keySet).withArgs(aList)
        .execute(myFunction.getId());
    }
```

Log these values within the function to help with tracing during development.
```
  public void execute(FunctionContext context) {
    ArrayList arguments = (ArrayList)(context.getArguments());
    String operation = (String)arguments.get(0);
    Object initiatingThreadID = arguments.get(1);
    String aStr = "In execute with context " + context + " and operation " + operation +
      " initiated by thread thr_" + initiatingThreadID + "_";
    for (int i = 2; i < arguments.size(); i++) {
      aStr = aStr + " additional arg: " + arguments.get(i);
    }
    System.out.println(aStr);
```
### Events
Since GemFire supports multiple CacheListeners, consider adding a LogListener which simply logs the relevant portion of the events as they are processed.  This provides another way to enable traceability in your application during the early stages of development.  For client/server applications, it can help to identify the originating member of an operation and the server that forwarded that event to a specific client.

If you are not using the CallbackArgument for your application, use the the callbackArgument to encode information about the caller or the data, which you can log in your LogListener.  

Events for operations initiated in the local vm are logged by the calling thread as shown below.  In this case vm_1_thr_10_edge4_w1-gst-dev18_10648.
```
[info 2014/05/30 14:04:10.674 PDT <vm_10_thr_10_edge4_w1-gst-dev18_10648> tid=0x51] Calling remove with key Object_395 value null, containsKey true, containsKeyOnServer true

[info 2014/05/30 14:04:10.674 PDT <vm_10_thr_10_edge4_w1-gst-dev18_10648> tid=0x51] Invoked util.SilenceListener for key Object_395: afterDestroy in edge4 event=EntryEventImpl[op=DESTROY;key=Object_395;oldValue=null;newValue=null;callbackArg=null;originRemote=false;originMember=w1-gst-dev18(edgegemfire4_w1-gst-dev18_10648:10648:loner):9766:1aa5f04e:edgegemfire4_w1-gst-dev18_10648;callbacksInvoked;version={v3; rv5; mbr=b90d31569c3243e8-8a2bcb43babe154a; ds=1; time=1401483850674; remote};id=EventID[threadID=2;sequenceID=0]]
     whereIWasRegistered: 10648
     event.getKey(): Object_395
     event.getOldValue(): null
     event.getNewValue(): null
     event.isLoad(): false
     event.isLocalLoad(): false
     event.isNetLoad(): false
     event.isNetSearch(): false
     event.isConcurrencyConflict(): false
     event.getDistributedMember(): w1-gst-dev18(edgegemfire4_w1-gst-dev18_10648:10648:loner):9766:1aa5f04e:edgegemfire4_w1-gst-dev18_10648
     event.getCallbackArgument(): null
     event.getRegion(): /testRegion
     event.isDistributed(): true
     event.isExpiration(): false
     event.isOriginRemote(): false
     Operation: DESTROY
     Operation.isLoad(): false
     Operation.isLocalLoad(): false
     Operation.isNetLoad(): false
     Operation.isNetSearch(): false
     Operation.isPutAll(): false
     Operation.isDistributed(): true
     Operation.isExpiration(): false

[info 2014/05/30 14:04:10.674 PDT <vm_10_thr_10_edge4_w1-gst-dev18_10648> tid=0x51] Done calling remove with key Object_395 value null, return value is true
```  

Events fired in remote members are fired on asynchronous threads.  In the case of clients, this asynchronous thread provides the identity of the server hosting the client's HARegionQueue.  In this case bridgegemfire5_w1_gst_dev18_79056.
```
[info 2014/05/30 14:04:10.674 PDT <Cache Client Updater Thread  on w1-gst-dev18(bridgegemfire5_w1-gst-dev18_79056:79056)<v60>:3080 port 27043> tid=0x26] Invoked util.SilenceListener for key Object_395: afterDestroy in edge3 event=EntryEventImpl[op=DESTROY;key=Object_395;oldValue=null;newValue=null;callbackArg=null;originRemote=true;originMember=w1-gst-dev18(:loner):9766:1aa5f04e;callbacksInvoked;version={v3; rv5; mbr=b90d31569c3243e8-8a2bcb43babe154a; ds=1; time=1401483850674; remote};id=EventID[threadID=2;sequenceID=0];isFromServer]
     whereIWasRegistered: 53128
     event.getKey(): Object_395
     event.getOldValue(): null
     event.getNewValue(): null
     event.isLoad(): false
     event.isLocalLoad(): false
     event.isNetLoad(): false
     event.isNetSearch(): false
     event.isConcurrencyConflict(): false
     event.getDistributedMember(): w1-gst-dev18(:loner):9766:1aa5f04e
     event.getCallbackArgument(): null
     event.getRegion(): /testRegion
     event.isDistributed(): true
     event.isExpiration(): false
     event.isOriginRemote(): true
     Operation: DESTROY
     Operation.isLoad(): false
     Operation.isLocalLoad(): false
     Operation.isNetLoad(): false
     Operation.isNetSearch(): false
     Operation.isPutAll(): false
     Operation.isDistributed(): true
     Operation.isExpiration(): false
```